# Whisper Document

# 1. ì‚¬ì „ ì§€ì‹

### **1.1. Whisper**

- OpenAI ê°œë°œ
- ASR ëª¨ë¸ â†’ STT, ì–¸ì–´ ê°ì§€, ë²ˆì—­ ë“±ì˜ ì‘ì—… ê°€ëŠ¥
- Transformer (Encoder-Decoder) ì•„í‚¤í…ì²˜ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë¨.
    
    ![image.png](Whisper%20Document%201501115c8eb880fdb0fbebd0220f7258/image.png)
    
- ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë°©ì–¸ ì§€ì› + ë°°ê²½ ì†ŒìŒì´ ë³µì¡í•œ í™˜ê²½ì—ì„œë„ ìƒëŒ€ì ìœ¼ë¡œ ì •í™•í•œ ê²°ê³¼ ì œê³µ
- ì‚¬ì´ì¦ˆë³„ë¡œ ì˜¤í”ˆì†ŒìŠ¤ ê³µê°œ
    
    
    | ëª¨ë¸ | íŒŒë¼ë¯¸í„° | URL |
    | --- | --- | --- |
    | whisper-large-v3 | 1.54B | [https://huggingface.co/openai/whisper-large-v3](https://huggingface.co/openai/whisper-large-v3) |
    | whisper-large-v3-turbo | 809M | [https://huggingface.co/openai/whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo) |
    | whisper-medium | 754M | [https://huggingface.co/openai/whisper-medium](https://huggingface.co/openai/whisper-medium) |
    | whisper-small | 242M | [https://huggingface.co/openai/whisper-small](https://huggingface.co/openai/whisper-small) |
    | whisper-base | 72.6M | [https://huggingface.co/openai/whisper-base](https://huggingface.co/openai/whisper-base) |
    | whisper-tiny | 37.8M | [https://huggingface.co/openai/whisper-tiny](https://huggingface.co/openai/whisper-tiny) |

### 1.2. Mel frequency

- ì¸ê°„ì˜ ì²­ê° íŠ¹ì„±ì„ ë°˜ì˜í•˜ì—¬ ìŒì„±ì„ ë¶„ì„í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ì£¼íŒŒìˆ˜ ì²™ë„
- ìŒì„± ì‹ í˜¸ ì²˜ë¦¬ì—ì„œ ì‚¬ëŒì˜ ì²­ê°ê³¼ ë” ìœ ì‚¬í•œ ë¶„ì„ì„ ìˆ˜í–‰

### **1.3. Mel frequency bins**

- Mel scaleë¡œ ë³€í™˜ëœ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì„ ë‚˜ëˆˆ êµ¬ê°„
- Mel frequency binsì˜ ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ë” ì„¸ë°€í•œ ì£¼íŒŒìˆ˜ ì •ë³´ê°€ í¬í•¨ë˜ì§€ë§Œ, ì—°ì‚° ë¹„ìš©ì´ ì¦ê°€í•¨
- Whisper ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” bins
    - *128 Mel bins*: large-v3, large-v3-turbo
    - *80 Mel bins*: large-v2, large-v1, medium, small, base, tiny

<aside>
âš ï¸

**ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì˜ Mel binsì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ë‹¤ë¥´ê²Œ í•´ì•¼í•œë‹¤.**

e.g. large-v3ì˜ FeatureExtractorë¡œ ì „ì²˜ë¦¬í•œ ë°ì´í„°(128 Mel bins)

      â†’ large-v3-turboì—ì„œ ì‚¬ìš© ê°€ëŠ¥ 

      â†’ large-v2ì—ì„œ ì‚¬ìš© ë¶ˆê°€ëŠ¥(ì±„ë„ ì—ëŸ¬ ë°œìƒ)

      â†’ mediumì—ì„œ ì‚¬ìš© ë¶ˆê°€ëŠ¥(ì±„ë„ ì—ëŸ¬ ë°œìƒ)

</aside>

### 1.4. Character Error Rate (CER)

- ë‘ ë¬¸ì¥ì„ ì² ì ê¸°ì¤€ìœ¼ë¡œ ë¹„êµí•œ ì—ëŸ¬ìœ¨ (ì£¼ë¡œ ëª¨ë¸ì˜ í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©ë¨)
- Word Error Rate (WER)ê³¼ ë¹„êµ
    
    <aside>
    ğŸ’¡
    
    **í•œêµ­ì–´ì˜ íŠ¹ì§•**
    
    - í˜•íƒœì†Œ ê¸°ë°˜ì˜ êµì°©ì–´ â†’ ì¡°ì‚¬ì™€ ì ‘ì‚¬ê°€ ì¡´ì¬í•¨
    - ë„ì–´ì“°ê¸° ê·œì¹™ì´ ë¹„êµì  ìœ ì—°
    </aside>
    
    - WERì€ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¹„êµí•˜ê¸° ë•Œë¬¸ì— ì‘ì€ ì² ìë‚˜ ì¡°ì‚¬, ë„ì–´ì“°ê¸°ì— ë¯¼ê°í•¨. í•œêµ­ì–´ì— ëŒ€í•´ ê³¼ë„í•˜ê²Œ ì˜¤ë¥˜ìœ¨ì´ ë†’ê²Œ ë‚˜ì˜¤ëŠ” ê²½í–¥ì´ ìˆìŒ
    - CERì€ ì² ì ë‹¨ìœ„ë¡œ ë¹„êµí•˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë³€í™”ì— ëœ ë¯¼ê°í•¨. ë”°ë¼ì„œ ì˜¤ë¥˜ë¥¼ ì ì ˆí•˜ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆìŒ

# 2. ë°ì´í„° ì¤€ë¹„

### 2.1. AIHub

1. [ì¤‘Â·ë…¸ë…„ì¸µ í•œêµ­ì–´ ë°©ì–¸ ë°ì´í„° (ì¶©ì²­ë„, ì „ë¼ë„, ì œì£¼ë„)](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71558)
2. [ì¤‘Â·ë…¸ë…„ì¸µ í•œêµ­ì–´ ë°©ì–¸ ë°ì´í„°(ê°•ì›ë„, ê²½ìƒë„)](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71517)
- ì´ ë°ì´í„° ìˆ˜: 700k

### 2.2 VOTE400

- [ì„¤ëª… ë° ë‹¤ìš´ë¡œë“œ](https://ai4robot.github.io/mindslab-etri-vote400/#)
- ìš©ëŸ‰: 30GB
- ëŒ€êµ¬(DG), ê²½ë‚¨(GN), ê°•ì›(GW), ì „ë‚¨(JN), ì„œìš¸(SE)

# 3. ë°ì´í„° ì „ì²˜ë¦¬

### 3.1. ê¸°ë³¸ ì „ì²˜ë¦¬

```python
from transformers import WhisperTokenizer, WhisperProcessor

# feature extractor, tokenizer ë¡œë“œ
model_name = "openai/whisper-large-v3"
feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)
tokenizer = WhisperTokenizer.from_pretrained(
    model_name, language="Korean", task="transcribe"
)

# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def prepare_dataset(batch):
    # load and resample audio data from 48 to 16kHz
    audio = batch["audio"]

    # compute log-Mel input features from input audio array
    batch["input_features"] = feature_extractor(
        audio["array"], sampling_rate=16000
    ).input_features[0]

    # encode target text to label ids
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch

# ì „ì²˜ë¦¬ ì ìš©
train_data = train_data.map(
    prepare_dataset,
    remove_columns=["audio", "sentence"],
    num_proc=2,
    desc="Preparing train dataset",
)
val_data = val_data.map(
    prepare_dataset,
    remove_columns=["audio", "sentence"],
    num_proc=2,
    desc="Preparing validation dataset",
)
test_data = test_data.map(
    prepare_dataset,
    remove_columns=["audio", "sentence"],
    num_proc=2,
    desc="Preparing test dataset",
)
```

### 3.2. SpecAugment

- ë°ì´í„° ì¦ê°• ê¸°ë²•
    
    ![image.png](Whisper%20Document%201501115c8eb880fdb0fbebd0220f7258/image%201.png)
    
    ![image.png](Whisper%20Document%201501115c8eb880fdb0fbebd0220f7258/image%202.png)
    
- ì¡ìŒì—ë„ Robustí•œ ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥
- ì°¸ê³ ìë£Œ: https://velog.io/@fbdp1202/Data-Augmentation-in-Audio-and-Speech-Feature-Drop-Aspect#filteraugment

```python
import torchaudio

# SpecAugment ì •ì˜
time_masking = torchaudio.transforms.TimeMasking(time_mask_param=30)
frequency_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=30)

# SpecAugment ì ìš© í•¨ìˆ˜
def apply_specaugment(features):
    features = torch.tensor(features).unsqueeze(0)
    features = time_masking(features)
    features = frequency_masking(features)
    return features.squeeze(0).numpy()

def apply_specaugment_to_dataset(batch):
    batch["input_features"] = apply_specaugment(batch["input_features"])
    return batch

# SpecAugmentê°€ ì ìš©ëœ ë°ì´í„°ì…‹ ìƒì„±
train_data = train_data.map(
    apply_specaugment_to_dataset, 
    num_proc=2,
    desc="Augmenting train dataset",
)
val_data = val_data.map(
    apply_specaugment_to_dataset, 
    num_proc=2, 
    desc="Augmenting validation dataset",
)
test_data = test_data.map(
    apply_specaugment_to_dataset,
    num_proc=2, 
    desc="Augmenting test dataset",
)
```

# 4. í•™ìŠµ

### íŒŒì¸íŠœë‹

```python
# 1. Data collator (í•™ìŠµì‹œ ë™ì  ì „ì²˜ë¦¬ìš©)
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        # Process input features
        input_features = [
            {"input_features": feature["input_features"]} for feature in features
        ]
        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )

        # get the tokenized label sequences
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        # pad the labels to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch

# 2. data collator ì´ˆê¸°í™”
data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# 3. í‰ê°€ ë©”íŠ¸ë¦­(CER) ì •ì˜
metric = evaluate.load("cer")
def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = tokenizer.pad_token_id
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    cer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"cer": cer}
    
# 4. ê°œì„ ëœ í•™ìŠµ ì¸ì ì„¤ì •
training_args = Seq2SeqTrainingArguments(
    output_dir=f"./{target_model_name}",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=64,
    learning_rate=5e-7,
    warmup_steps=250,
    num_train_epochs=3,
    fp16=False,
    bf16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=32,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=250,
    eval_steps=250,
    logging_steps=10,
    report_to=["wandb"],
    load_best_model_at_end=True,
    metric_for_best_model="cer",
    greater_is_better=False,
    push_to_hub=False,
    dataloader_num_workers=2,  # ë°ì´í„° ë¡œë”© ì„±ëŠ¥ í–¥ìƒ
)

# 5. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° í•™ìŠµ
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=train_data,
    eval_dataset=val_data,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)

# 6. í”„ë¡œì„¸ì„œ ë¯¸ë¦¬ ì €ì¥
processor.save_pretrained(training_args.output_dir)

# 7. í•™ìŠµ
trainer.train()
```

### ì„¤ì •ë³„ í•™ìŠµ ê²°ê³¼

- ë°ì´í„°
    1. AIHub + Vote400 â‡’ **AV**
    2. AIHub + Vote400 + SpecAugment â‡’ **AVS**
    3. AIHub + Vote400 + SpecAugment + General â‡’ **AVSG**
- ëª¨ë¸
    1. openai/whisper-large-v3 â‡’ **L**
    2. openai/whisper-medium â‡’ **M**
    3. openai/whisper-small â‡’ **S**
    4. openai/whisper-base â‡’ **B**
    5. openai/whisper-tiny â‡’ **T**

| (CER) | ì „ë¼ë„ | ì œì£¼ë„ | ì¶©ì²­ë„ | ê²½ìƒë„ | ê°•ì›ë„ | ëŒ€êµ¬(DG) | ê²½ë‚¨(GN) | ê°•ì›(GW) | ì „ë‚¨(JN) | ì„œìš¸(SE) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **L + AV** | **0.0556** | **0.0632** | **0.0579** | **0.0710** | **0.0476** | **0.0718** | **0.0275** | **0.0139** | **0.0372** | **0.0201** |
| M + AV | 0.1352 | 0.1103 | 0.0753 | 0.1016 | 0.0840 | 0.0455 | 0.0463 | 0.0346 | 0.0397 | 0.0359 |
| S + AV | 0.4263 | 0.2241 | 0.1235 | 0.1777 | 0.1348 | 0.0668 | 0.0729 | 0.0590 | 0.0773 | 0.0675 |
| B + AV | 0.4696 | 0.3454 | 0.2071 | 0.3084 | 0.3076 | 0.1362 | 0.1510 | 0.1254 | 0.1278 | 0.1037 |
| T + AV | 0.5709 | 0.5023 | 0.3020 | 0.4079 | 0.3466 | 0.2173 | 0.2121 | 0.1799 | 0.2085 | 0.1835 |

| (CER) | ì „ë¼ë„ | ì œì£¼ë„ | ì¶©ì²­ë„ | ê²½ìƒë„ | ê°•ì›ë„ | ëŒ€êµ¬(DG) | ê²½ë‚¨(GN) | ê°•ì›(GW) | ì „ë‚¨(JN) | ì„œìš¸(SE) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| L + AV | **0.0556** | **0.0632** | **0.0579** | **0.0710** | **0.0476** | **0.0718** | **0.0275** | **0.0139** | **0.0372** | **0.0201** |
| L + AVS |  |  |  |  |  |  |  |  |  |  |
| L + AVSG |  |  |  |  |  |  |  |  |  |  |
| S + AV | 0.4263 | 0.2241 | 0.1235 | 0.1777 | 0.1348 | 0.0668 | 0.0729 | 0.0590 | 0.0773 | 0.0675 |
| S + AVS | 0.4356 | 0.2066 | 0.1326 | 0.1892 | 0.1350 | 0.0725 | 0.0800 | 0.0671 | 0.7780 | 0.0751 |

# 5. ì‚¬ìš©

### ë¡œì»¬ ì‹¤í–‰

- whisper.py
    - ì „ì‚¬ ë°ì´í„°ì™€ ëª¨ë¸ ì¶”ë¡  ê²°ê³¼ ë¹„êµ
    - main ë¶€ë¶„ë§Œ ë°”ê¾¸ë©´ ì‚¬ìš©ê°€ëŠ¥
    
    ```python
    import torch
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    from datasets import load_dataset
    import os, json
    import evaluate
    
    os.environ["CUDA_VISIBLE_DEVICES"] = "3"
    
    # CER(Character Error Rate) ë©”íŠ¸ë¦­ ë¡œë“œ
    cer_metric = evaluate.load("cer")
    
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    
    ft_model = "local path or HuggingFace name"
    model_id = "openai/whisper-large-v3"
    
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        ft_model, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    
    processor = AutoProcessor.from_pretrained(model_id)
    
    pipe = pipeline(
        "automatic-speech-recognition",
        model=model,
        tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor,
        torch_dtype=torch_dtype,
        device=device,
    )
    
    def whisper(audio_file, input_filename):
        save_dir = "temp"
    
        temp_audio_path = os.path.join(save_dir, input_filename)
    
        with open(temp_audio_path, "wb") as f:
            f.write(audio_file)
    
        try:
            # pipe í•¨ìˆ˜ í˜¸ì¶œ ë° ê²½ë¡œ ì „ë‹¬
            result = pipe(temp_audio_path)
        finally:
            # ì‚¬ìš© í›„ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_audio_path):
                os.remove(temp_audio_path)
    
        return result
    
    if __name__ == "__main__":
        file_path = "script"
        for file in os.listdir(file_path):
            with open(os.path.join(file_path, file), "r") as json_file:
                data = json.load(json_file)
    
            mp3_file_name = data["fileName"] + ".wav"
            script = data["script"]["value"]
    
            try:
                with open(os.path.join("data", mp3_file_name), "rb") as mp3:
                    mp3_bytes = mp3.read()
            except:
                continue   
            result = whisper(mp3_bytes)
    
            cer = cer_metric.compute(predictions=[result["text"]], references=[script])
    
            # ê²°ê³¼ ì¶œë ¥
            print(f"File: {file}")
            print(f"Reference (script): {script}")
            print(f"Prediction (Whisper): {result['text']}")
            print(f"CER: {cer:.2f}\n")
    
    ```
    

### RestAPI í„°ë„ë§

- server.py
    - whisper.pyì—ì„œ ì •ì˜í•œ whisper í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ API í˜¸ì¶œ ì²˜ë¦¬
    - webmìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” íŒŒì¼ì€ mp3ë¡œ ë³€ê²½í•´ì„œ ì‚¬ìš©
    
    ```python
    from fastapi import FastAPI, HTTPException, UploadFile, File
    from fastapi.responses import RedirectResponse
    from fastapi.middleware.cors import CORSMiddleware
    from langserve import add_routes
    from pydantic import BaseModel
    from whisper import whisper
    import ffmpeg, os
    
    app = FastAPI()
    
    # Set all CORS enabled origins
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
        expose_headers=["*"],
    )
    
    # webm -> mp3 íŒŒì¼
    async def convert_webm_to_mp3(input_bytes: bytes, input_filename: str) -> bytes:
        # webm íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        input_path = f"temp/{input_filename}"
        with open(input_path, "wb") as f:
            f.write(input_bytes)
        
        # ë³€í™˜ëœ mp3 íŒŒì¼ì˜ ê²½ë¡œ ì„¤ì •
        output_path = input_path.replace(".webm", ".mp3")
        
        # ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ ì‘ì—… ìˆ˜í–‰
        ffmpeg.input(input_path).output(output_path).run()
    
        # ë³€í™˜ëœ mp3 íŒŒì¼ì„ ë°”ì´íŠ¸ë¡œ ì½ê¸°
        with open(output_path, "rb") as f:
            mp3_bytes = f.read()
    
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(input_path)
        os.remove(output_path)
    
        return mp3_bytes
    
    @app.post("/whisper")
    async def chain_start(audio: UploadFile = File(...)):
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ë°”ì´íŠ¸ë¡œ ì½ìŒ
            audio_bytes = await audio.read()
            filename = audio.filename
    
            # íŒŒì¼ í™•ì¥ìê°€ webmì¼ ê²½ìš° mp3ë¡œ ë³€í™˜
            if filename.endswith(".webm"):
                audio_bytes = await convert_webm_to_mp3(audio_bytes, filename)
            
            # whisper í•¨ìˆ˜ì— íŒŒì¼ ë°”ì´íŠ¸ ì „ë‹¬
            result = whisper(audio_bytes, filename)
            return {"result": result}
        
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
        
    
    if __name__ == "__main__":
        import uvicorn
    
        uvicorn.run(app, host="0.0.0.0", port=8001)
    ```
    
- https://ngrok.com/
    - í„°ë„ë§ì„ ìœ„í•œ ë¬´ë£Œ URL ìƒì„±
    - í„°ë„ë§
        
        ```python
        # í„°ë„ë§ ì˜ˆì‹œ ì½”ë“œ (ì‚¬ìš© ë¶ˆê°€
        ngrok http --domain=sawfish-charming-pangolin.ngrok-free.app 8001
        ```
        
    - API í˜¸ì¶œ ê°€ëŠ¥
        
        ```python
        curl -X POST "https://sawfish-charming-pangolin.ngrok-free.app/whisper" \
          -H "Content-Type: multipart/form-data" \
          -F "audio=@audio_test.mp3"
        ```
        

---

<aside>
ğŸ”–

**Reference**

@misc{radford2022whisper,
doi = {10.48550/ARXIV.2212.04356},
url = {[https://arxiv.org/abs/2212.04356](https://arxiv.org/abs/2212.04356)},
author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
title = {Robust Speech Recognition via Large-Scale Weak Supervision},
publisher = {arXiv},
year = {2022},
copyright = {[arXiv.org](http://arxiv.org/) perpetual, non-exclusive license}
}

</aside>

### ì‘ì„±ì

ë™êµ­ëŒ€í•™êµ
ì»´í“¨í„°ê³µí•™ì „ê³µ
ì´ë¦„: ì •ìˆ˜ì±„
email: jeongsuchae9211@gmail.com